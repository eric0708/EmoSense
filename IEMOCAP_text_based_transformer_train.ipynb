{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR Based Speech Emotion Recognition on IEMOCAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to help construct dictionary of sentence id to text using transcriptions files\n",
    "def id_2_text_construct(id_2_text, transcriptions_dir):\n",
    "    for filename in os.listdir(transcriptions_dir):\n",
    "        if filename.split('.')[-1] != 'txt':\n",
    "            continue\n",
    "\n",
    "        with open(os.path.join(transcriptions_dir, filename), 'r') as file:\n",
    "            for line in file:\n",
    "                line_split = line.split()\n",
    "\n",
    "                if line_split[0].startswith('Ses'):\n",
    "                    id_2_text[line_split[0]] = ' '.join(line_split[2:])\n",
    "                    \n",
    "    return id_2_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to help construct dictionary of sentence id to label using evaluation files\n",
    "def id_2_label_construct(id_2_label, evaluation_dir):\n",
    "    for filename in os.listdir(evaluation_dir):\n",
    "        if filename.split('.')[-1] != 'txt':\n",
    "            continue\n",
    "\n",
    "        with open(os.path.join(evaluation_dir, filename), 'r') as file:\n",
    "            for line in file:\n",
    "                line_split = line.split()\n",
    "\n",
    "                if len(line_split) >= 4 and line_split[3].startswith('Ses'):\n",
    "                    sentence_id = line_split[3]\n",
    "                    label = line_split[4]\n",
    "                    if label != 'xxx' and label != 'oth':\n",
    "                        id_2_label[sentence_id] = label\n",
    "                        \n",
    "    return id_2_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dictionaries\n",
    "id_2_text = {}\n",
    "id_2_label = {}\n",
    "\n",
    "# iterate through all the session directories\n",
    "for session_num in range(1, 6):\n",
    "    session_dir = 'Dataset/IEMOCAP/Session{}'.format(session_num)\n",
    "    transcriptions_dir = os.path.join(session_dir, 'dialog/transcriptions')\n",
    "    evaluation_dir = os.path.join(session_dir, 'dialog/EmoEvaluation')\n",
    "\n",
    "    id_2_text = id_2_text_construct(id_2_text, transcriptions_dir)\n",
    "    id_2_label = id_2_label_construct(id_2_label, evaluation_dir)\n",
    "\n",
    "# iterate through all id_2_label sentence ids and locate corresponding text\n",
    "idx_2_label = {}\n",
    "label_2_idx = {}\n",
    "text_and_label = []\n",
    "\n",
    "idx_count = 0\n",
    "\n",
    "for sentence_id in id_2_label:\n",
    "    label = id_2_label[sentence_id]\n",
    "    text = id_2_text[sentence_id]\n",
    "\n",
    "    if label not in label_2_idx:\n",
    "        label_2_idx[label] = idx_count\n",
    "        idx_2_label[idx_count] = label\n",
    "        idx_count += 1\n",
    "    \n",
    "    text_and_label.append((text, label_2_idx[label]))\n",
    "\n",
    "# save the dictionaries and lists to dataset directory\n",
    "dataset_dir = 'Dataset/IEMOCAP'\n",
    "\n",
    "with open(os.path.join(dataset_dir, 'idx_2_label.json'), 'w') as json_file:\n",
    "    json.dump(idx_2_label, json_file)\n",
    "\n",
    "with open(os.path.join(dataset_dir, 'label_2_idx.json'), 'w') as json_file:\n",
    "    json.dump(label_2_idx, json_file)\n",
    "\n",
    "with open(os.path.join(dataset_dir, 'text_and_label.json'), 'w') as json_file:\n",
    "    json.dump(text_and_label, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Train and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the list from the JSON file and separate text and label data\n",
    "with open(os.path.join(dataset_dir, 'text_and_label.json'), 'r') as json_file:\n",
    "    text_and_label = json.load(json_file)\n",
    "\n",
    "text_data = []\n",
    "label_data = []\n",
    "\n",
    "for text, label in text_and_label:\n",
    "    text_data.append(text)\n",
    "    label_data.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data into train and valid datasets\n",
    "train_texts, valid_texts, train_labels, valid_labels = train_test_split(text_data, label_data, test_size=0.12, random_state=42, stratify=label_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset for IEMOCAP text emotion classification\n",
    "class IEMOCAP_Text_Dataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load idx to label dictionary\n",
    "with open(os.path.join(dataset_dir, 'idx_2_label.json'), 'r') as json_file:\n",
    "    idx_2_label = json.load(json_file)\n",
    "\n",
    "# define tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(idx_2_label))\n",
    "\n",
    "# define dataset\n",
    "train_dataset = IEMOCAP_Text_Dataset(train_texts, train_labels, tokenizer)\n",
    "valid_dataset = IEMOCAP_Text_Dataset(valid_texts, valid_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training parameters\n",
    "batch_size = 8\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Train for Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 829/829 [05:13<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Validation for Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:08<00:00, 13.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 1.5546, Train Accuracy: 0.4066, Valid Loss: 1.3347, Valid Accuracy: 0.4978\n",
      "\n",
      "Start Train for Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 829/829 [05:09<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Validation for Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:08<00:00, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Train Loss: 1.1506, Train Accuracy: 0.5707, Valid Loss: 1.2335, Valid Accuracy: 0.5531\n",
      "\n",
      "Start Train for Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 829/829 [04:40<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Validation for Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:08<00:00, 13.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Train Loss: 0.9412, Train Accuracy: 0.6509, Valid Loss: 1.3171, Valid Accuracy: 0.5254\n",
      "\n",
      "Start Train for Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 829/829 [05:18<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Validation for Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:08<00:00, 13.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Train Loss: 0.7995, Train Accuracy: 0.7008, Valid Loss: 1.2818, Valid Accuracy: 0.5642\n",
      "\n",
      "Start Train for Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 829/829 [05:11<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Validation for Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:08<00:00, 13.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Train Loss: 0.6819, Train Accuracy: 0.7544, Valid Loss: 1.3970, Valid Accuracy: 0.5465\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# set up optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# training loop\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train\n",
    "    print(\"Start Train for Epoch {}\".format(epoch+1))\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct_preds = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        labels = inputs['labels'].to(device)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # calculate train accuracy\n",
    "        _, train_predictions = torch.max(logits, dim=1)\n",
    "        train_correct_preds += torch.sum(train_predictions == labels).item()\n",
    "\n",
    "    # validation\n",
    "    print(\"Start Validation for Epoch {}\".format(epoch+1))\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_correct_preds = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_dataloader):\n",
    "            inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            labels = inputs['labels'].to(device)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            # calculate validation accuracy\n",
    "            _, valid_predictions = torch.max(logits, dim=1)\n",
    "            valid_correct_preds += torch.sum(valid_predictions == labels).item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    train_accuracy = train_correct_preds / len(train_dataset)\n",
    "    avg_valid_loss = valid_loss / len(valid_dataloader)\n",
    "    valid_accuracy = valid_correct_preds / len(valid_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Valid Loss: {avg_valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Models/finetuned_roberta_IEMOCAP/tokenizer_config.json',\n",
       " 'Models/finetuned_roberta_IEMOCAP/special_tokens_map.json',\n",
       " 'Models/finetuned_roberta_IEMOCAP/vocab.json',\n",
       " 'Models/finetuned_roberta_IEMOCAP/merges.txt',\n",
       " 'Models/finetuned_roberta_IEMOCAP/added_tokens.json')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify the directory to save the models\n",
    "model_dir = 'Models'\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "finetuned_model_dir = os.path.join(model_dir, 'finetuned_roberta_IEMOCAP')\n",
    "\n",
    "# save model and tokenizer to specified directory\n",
    "model.save_pretrained(finetuned_model_dir)\n",
    "tokenizer.save_pretrained(finetuned_model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
